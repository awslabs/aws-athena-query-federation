# Athena Federation Testing README

### Next Steps
1. Adding more data sources
2. Integrate with Github for PR test

## Overview
This package serves as a full suite of util you will need when working with external data sources.
This test framework provide below functionality, we will break it down as few category

### TPCDS Data generation
1. Generate TPCDS data into S3 bucket.
 
### External data source
1. Boostrap infrastructure for external datasource
   1. VPC (IF required)
   2. S3 buckets: 1)Spill bucket, 2)Result bucket, 3)Glue ETL bucket
   3. External data source.
2. Loading data into external datasource
   1. Loading Data from s3 parquet to external data source through Glue ETL
   2. Loading data directly from local through dataframe to data source

### Connectors
1. Creating a prod connector
2. Update connector with local changes
3. Migrate connector to LakeFormation and Glue Federation
    
### Testing
We provide testing for both from Athena Datacatalog and Glue datacatalog
1. Perform testing on metadata call
2. Perform simple select query testing
3. Perform complex select query <- provide metric that can be used to measure performance

-----
## Setup:
### Pre-rquesite
**This section is for first time user** 

Under Root: 
```
Python at least 3.12
pip at least 24.3

python -m venv perf_testing
source perf_testing/bin/activate
pip install -r requirements.txt
```

Manual Steps:

This testing framework will boostrap resources for you, hence, this required a `Master_key` for all databases generated.
You will need to fill config.properties.

1. `tpcds_scale_factor` -> specify factor, example: tpcds1, tpcds30, tpcds100
2. `lf_admin_role_arn` -> specify if you want to use migrate catalog from Athena to LakeFormation
3. =Required a secret from secret manager for  `rds_secret_name`.

Format as below
```
"{username":"<Your password, chose anything but not admin>","password":"< #Passowrd must contains capital case>"}
```
above limitation due to certain database requirements. In order to make this generic for all data sources, please follow above 

2. Required role, you will need to provide below role arn
```
lf_admin_role_arn <- Role arn that you will be vending credential based on. Please make sure this role has access to S3 spill bucket and Lambda function
glue_job_role_arn <- Role that we used to run Glue job ETL 
```
Please make sure above has trust entity to LakeFormation and Glue

-----
## TPCDS Data generation:
1. https://aws.amazon.com/marketplace/server/procurement?productId=94a13e9e-e2c3-4b49-8fcb-7a4713820307 go to this website and click accept. 

2. It is free, you will need this to generate data.

The data will be generated by Glue job ETL and store at below s3 location `s3://athena-federation-integ-glue-bucket-{region}-{account_number}/data/<tpcds_factor>`

Example: s3://athena-federation-integ-glue-bucket-us-west-2-0123456789/data/tpcds_1/call_center/

-----
## External data source
Currently, the framework only support ["dynamodb", "mysql", "documentdb","postgresql", "redshift", "sqlserver", "oracle"]

The data will be loading to external source as below format `<top_level_in_external_source>.tpcds<factor>.table_name`

Example: tpcds.tpcds1.customer

This way we can keep all the format consistent and make our testing easier.

### DynamoDB:

Dynamodb does not have a concept of database/schema, hence, we will be loading table name as `tpcds1_customer`


#### Write capacity limit: 
If during load data step you see reach write capacity issue, you can change the old table capacity to on-demand. Example if you already load tpcd1 & tpcds10 and want to load tpcds100, you will need to change tpcds1&tpcds10 capacity to on-demand.

Another way to do it is to increase write capacity in your account.

### Oracle:
In order to start up an oracle, you will require to have your Oracle license under your account


### Redshift:
If your redshift cluster can't turn on public access. it is very likely due to below
common issue: https://repost.aws/knowledge-center/redshift-serverless-publicly-accessible

### OpenSearch:
If your glue job ETL experience failure and glue ETL error log showing index:{}. Likely due to throttling or hitting ops limit. 

Increase `Iops` and `Throughput` volume and try again. 

### BigQuery:

We only delete what is the current select scale factor to avoid accidental deletion for rest of prefix start with*

#### BigQuery set up
1. Please manually create a project in big query (and create account).
2. Create a key for big query service https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-connect-bigquery-home.html


### Snowflake:

We only delete what is the current select scale factor to avoid accidental deletion for rest of prefix start with*

#### Snowflake set up
1. Please manually create a snowflake account
2. Manually create a snowflake database (catalog as in aws)
3. Create a key for snowflake

-----
## Connectors
This framework will generate connector through Athena:CreateDataCatalog. 

Athena DataCatalog name will be as follow: `athena_integ_<source_type>`

If you are running with FGAC test, your catalog will be migrate over to GDC/LF and your Glue datacatalog will follow this format `athena_glue_integ_<source_type>`

### Catalog Migration

#### Migrate Athena Catalog to Glue Catalog
```
python infra_main.py --action migrate_to_glue_catalog --resources <source_type>
```
This migrates an Athena catalog to Glue with LakeFormation integration.

#### Migrate Glue Catalog to Athena Catalog
```
python infra_main.py --action migrate_to_athena_catalog --resources <source_type>
```
This will:
1. Deregister the Glue connection from LakeFormation
2. Delete the Glue catalog
3. Create an Athena catalog with Type='LAMBDA' using the same Lambda function

-----
## Testing
Testing can be done against different type of catalogs 1. Athena DataCatalog, 2. Glue DataCatalog

Test results can be optionally saved to CSV files using the `--output_file_prefix` parameter. Each resource will generate a separate file: `<prefix>_<resource>.csv`

Contains 3 type of testing 

### Metadata 
1. GetCatalog API call
2. ListDatabase API call
3. List Tables API call
4. GetTable API call

**For select query below: we will gather below metrics**
[`EngineExecutionTimeInMillis`, `QueryPlanningTimeInMillis`, `DataScannedInBytes`]
### Simple Select
1. ```SELECT * FROM athena_integ_dynamodb.default.<table_name> LIMIT 1```
2. ```SELECT * FROM athena_integ_dynamodb.default.<table_name> ```


### Complex select
Base line:
```
SELECT * FROM athena_integ_dynamodb.default.customer
```

Select with predicate:
```
SELECT * FROM athena_integ_dynamodb.default.customer where c_customer_sk = 65
```

select with limit:
```
SELECT * FROM athena_integ_dynamodb.default.customer LIMIT 1
```

select with complex expression:
```
SELECT * FROM athena_integ_dynamodb.default.customer where (c_customer_sk + 5) > 500
```

select with count:
```
SELECT count(*) FROM athena_integ_dynamodb.default.customer,customer
```

Dynamic filter:
TPCDS query with q20, q32

-----
## Example Usage

Generate TPCDS data using Glue ETL connector
```
python infra_main.py --action generate_tpcds;
```

General
```
python infra_main.py --action ["generate_tpcds","create_infra","destroy_infra","load_data","delete_resource","create_connector","update_connector","migrate_to_glue_catalog","migrate_to_athena_catalog"] --resources ["dynamodb","mysql","documentdb","postgresql","redshift","sqlserver","elasticsearch","bigquery","snowflake"]
```

Create Infra for all connectors
```
Create all data sources
python infra_main.py --action create_infra; 

Specific resource
python infra_main.py --action create_infra [--resources ["dynamodb", "mysql", "documentdb","postgresql", "redshift", "sqlserver", "elasticsearch", "bigquery", "snowflake"]]
```

Load Data to data source
```
python infra_main.py --action load_data;
```

Create_connector with public API
```
python infra_main.py --action create_connector;
```

Update connector with local changes

*Notes: Please have docker running*
```
python infra_main.py --action update_connector;
```

Run test:
```
Test Athena catalog
python testing_main.py --action test_athena

Test Glue Catalog
python testing_main.py --action test_glue

Test Everything
python testing_main.py --action release_test

Save test results to CSV files (one per resource)
python testing_main.py --action test_athena --output_file_prefix results
python testing_main.py --action test_glue --output_file_prefix results
python testing_main.py --action release_test --output_file_prefix results

Test specific resources
python testing_main.py --action test_athena --resources dynamodb,mysql
```